

from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy
# æ ¸å¿ƒå¯¼å…¥ï¼šcrawl4ai 0.8.0 ä¸“å±LLMConfig
from crawl4ai import LLMConfig
from dotenv import load_dotenv
import os
import asyncio

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆè¯»å–æ™ºè°±å¯†é’¥/æ¨¡å‹ï¼‰
load_dotenv()
ZHIPU_API_KEY = "cd72ca69e2b944cebf6523b0750f0c36.0EA6Wo4y6agqsSj7"
ZHIPU_MODEL ="ZHIPU_MODEL/glm-4-flash"
# æ™ºè°±OpenAIå…¼å®¹æ¥å£åœ°å€ï¼ˆå›ºå®šï¼Œåˆ‡å‹¿ä¿®æ”¹ï¼‰
ZHIPU_BASE_URL = "https://open.bigmodel.cn/api/paas/v4/"

# è‡ªå®šä¹‰æ™ºè°±æ¸…æ´—æç¤ºè¯ï¼ˆå¯æŒ‰éœ€æ±‚ä¿®æ”¹è§„åˆ™ï¼‰
CLEAN_PROMPT = """
è¯·ä½ ä½œä¸ºä¸“ä¸šç½‘é¡µå†…å®¹æ¸…æ´—å¸ˆï¼Œå¤„ç†çˆ¬å–çš„åŸå§‹å†…å®¹ï¼Œä¸¥æ ¼éµå®ˆï¼š
1. å½»åº•åˆ é™¤å¹¿å‘Šã€å¯¼èˆªã€ä¾§è¾¹æ ã€è¯„è®ºåŒºã€ç›¸å…³æ¨èç­‰æ‰€æœ‰å†—ä½™ä¿¡æ¯ï¼›
2. ä»…ä¿ç•™æ ¸å¿ƒæœ‰æ•ˆå†…å®¹ï¼Œä¿ç•™æ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨çš„åŸå§‹é€»è¾‘ç»“æ„ï¼›
3. ç”¨Markdownæ ¼å¼åŒ–æ¸…æ´—åçš„å†…å®¹ï¼Œæ ‡é¢˜åˆ†çº§ã€æ®µè½åˆ†è¡Œï¼Œåˆ é™¤æ— æ•ˆç©ºè¡Œï¼›
4. åªè¾“å‡ºæ¸…æ´—åçš„çº¯å‡€å†…å®¹ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–è§£é‡Šã€å¤‡æ³¨ã€‚
"""

async def crawl_and_clean_with_zhipu(url: str):
    """çˆ¬å–ç½‘é¡µ+æ™ºè°±å¤§æ¨¡å‹æ¸…æ´—ï¼ˆæœ€ç»ˆä¿®æ­£ç‰ˆï¼Œé€‚é…crawl4ai 0.8.0ï¼‰"""
    async with AsyncWebCrawler(verbose=True) as crawler:
        try:
            # æ ¸å¿ƒä¿®æ­£ï¼šLLMConfigç”¨llm_modelæŒ‡å®šæ¨¡å‹ï¼ˆæ›¿ä»£åŸmodelï¼Œè§£å†³æŠ¥é”™ï¼‰
            zhipu_llm_config = LLMConfig(
                llm_model=ZHIPU_MODEL,      # ã€å”¯ä¸€ä¿®æ”¹ç‚¹ã€‘model â†’ llm_model
                base_url=ZHIPU_BASE_URL,    # æ™ºè°±æ¥å£åœ°å€ï¼ˆä¸å˜ï¼‰
                api_key=ZHIPU_API_KEY,      # æ™ºè°±APIå¯†é’¥ï¼ˆä¸å˜ï¼‰
                temperature=0.1,            # æ¸…æ´—å†…å®¹å»ºè®®0.0-0.3ï¼Œè¾“å‡ºæ›´ç¨³å®š
                max_tokens=8192,            # æœ€å¤§è¾“å‡ºtokenï¼Œé•¿é¡µé¢å¯è°ƒå¤§
                request_timeout=30,         # LLMè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé¿å…å¡é¡¿
            )

            # é…ç½®LLMæ¸…æ´—ç­–ç•¥ï¼Œä¼ å…¥å°è£…å¥½çš„LLMConfig
            llm_strategy = LLMExtractionStrategy(
                llm_config=zhipu_llm_config,
                prompt=CLEAN_PROMPT
            )

            # æ‰§è¡Œçˆ¬å–+æ¸…æ´—
            result = await crawler.arun(
                url=url,
                extraction_strategy=llm_strategy,
                # çˆ¬è™«åŸºç¡€é…ç½®ï¼ˆæŒ‰éœ€è°ƒæ•´ï¼‰
                bypass_cache=True,
                render_js=False,  # åŠ¨æ€é¡µé¢æ”¹Trueï¼Œé™æ€é¡µé¢ä¿æŒFalseæ›´é«˜æ•ˆ
                wait_for_navigation=True,
                timeout=60,
                # è‡ªå®šä¹‰UAï¼Œé™ä½åçˆ¬æ¦‚ç‡
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36"
            )

            # è¾“å‡ºç»“æœ+ä¿å­˜åˆ°æœ¬åœ°
            print("="*100)
            print("âœ… çˆ¬å–+æ¸…æ´—æˆåŠŸï¼æ™ºè°±æ¸…æ´—åçš„å†…å®¹ï¼š\n")
            print(result.extracted_content)
            print("="*100)

            # ä¿å­˜æ¸…æ´—åçš„å†…å®¹åˆ°Markdownæ–‡ä»¶
            with open("æ™ºè°±æ¸…æ´—åçš„ç½‘é¡µå†…å®¹.md", "w", encoding="utf-8") as f:
                f.write(result.extracted_content)
            print("ğŸ“„ å†…å®¹å·²ä¿å­˜è‡³ï¼šæ™ºè°±æ¸…æ´—åçš„ç½‘é¡µå†…å®¹.md")

            return result.extracted_content

        except Exception as e:
            print(f"âŒ çˆ¬å–/æ¸…æ´—å¤±è´¥ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{str(e)}")
            return None

# æµ‹è¯•ä¸»å‡½æ•°
if __name__ == "__main__":
    # æ›¿æ¢ä¸ºä½ çš„ç›®æ ‡URLï¼ˆé™æ€/åŠ¨æ€é¡µé¢å‡å¯ï¼‰
    TARGET_URL = "https://www.infoq.cn/article/2025-09-python-crawl4ai-practice"
    asyncio.run(crawl_and_clean_with_zhipu(TARGET_URL))