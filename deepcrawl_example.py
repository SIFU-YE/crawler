import asyncio
import time

from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.filters import (
    FilterChain,
    URLPatternFilter,
    DomainFilter,
    ContentTypeFilter,
    ContentRelevanceFilter,
    SEOFilter,
)
from crawl4ai.deep_crawling.scorers import (
    KeywordRelevanceScorer,
)


# 1ï¸âƒ£ åŸºç¡€æ·±åº¦çˆ¬å–è®¾ç½®
async def basic_deep_crawl():
    """
    ç¬¬ä¸€éƒ¨åˆ†: åŸºç¡€æ·±åº¦çˆ¬å–è®¾ç½® - æ¼”ç¤ºç®€å•çš„ä¸¤çº§æ·±åº¦çˆ¬å–ã€‚

    æœ¬å‡½æ•°å±•ç¤ºï¼š
    - å¦‚ä½•è®¾ç½® BFSDeepCrawlStrategyï¼ˆå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼‰
    - è®¾ç½®æ·±åº¦å’ŒåŸŸåå‚æ•°
    - å¤„ç†ç»“æœä»¥æ˜¾ç¤ºå±‚çº§ç»“æ„
    """
    print("\n===== åŸºç¡€æ·±åº¦çˆ¬å–è®¾ç½® =====")

    # ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢ç­–ç•¥é…ç½®2çº§æ·±åº¦çˆ¬å–
    # max_depth=2 è¡¨ç¤ºï¼šåˆå§‹é¡µé¢ï¼ˆæ·±åº¦0ï¼‰+ 2ä¸ªé¢å¤–å±‚çº§
    # include_external=False è¡¨ç¤ºï¼šåªçˆ¬å–åŒä¸€åŸŸåå†…çš„é“¾æ¥
    config = CrawlerRunConfig(
        deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=2, include_external=False),
        scraping_strategy=LXMLWebScrapingStrategy(),
        verbose=True,  # çˆ¬å–è¿‡ç¨‹ä¸­æ˜¾ç¤ºè¿›åº¦
    )

    async with AsyncWebCrawler() as crawler:
        start_time = time.perf_counter()
        results = await crawler.arun(url="https://wiki.smartbi.com.cn/pages/viewpage.action?smt_poid=23&pageId=111891997", config=config)

        # æŒ‰æ·±åº¦åˆ†ç»„ç»“æœä»¥å¯è§†åŒ–çˆ¬å–æ ‘
        pages_by_depth = {}
        for result in results:
            depth = result.metadata.get("depth", 0)
            if depth not in pages_by_depth:
                pages_by_depth[depth] = []
            pages_by_depth[depth].append(result.url)

        print(f"âœ… æ€»å…±çˆ¬å–äº† {len(results)} ä¸ªé¡µé¢")

        # æŒ‰æ·±åº¦æ˜¾ç¤ºçˆ¬å–ç»“æ„
        for depth, urls in sorted(pages_by_depth.items()):
            print(f"\nDepth {depth}: {len(urls)} pages")
            # æ˜¾ç¤ºæ¯ä¸ªæ·±åº¦çš„å‰3ä¸ªURLä½œä¸ºç¤ºä¾‹
            for url in urls[:3]:
                print(f"  â†’ {url}")
            if len(urls) > 3:
                print(f"  ... è¿˜æœ‰ {len(urls) - 3} ä¸ª")

        print(
            f"\nâœ… æ€§èƒ½: {len(results)} ä¸ªé¡µé¢ç”¨æ—¶ {time.perf_counter() - start_time:.2f} ç§’"
        )

# 2ï¸âƒ£ æµå¼ vs éæµå¼æ‰§è¡Œ
async def stream_vs_nonstream():
    """
    ç¬¬äºŒéƒ¨åˆ†: æ¼”ç¤ºæµå¼å’Œéæµå¼æ‰§è¡Œä¹‹é—´çš„åŒºåˆ«ã€‚

    éæµå¼ï¼šç­‰å¾…æ‰€æœ‰ç»“æœå®Œæˆåå†å¤„ç†
    æµå¼ï¼šç»“æœå¯ç”¨æ—¶ç«‹å³å¤„ç†
    """
    print("\n===== æµå¼ vs éæµå¼æ‰§è¡Œ =====")

    # ä¸¤ä¸ªç¤ºä¾‹çš„é€šç”¨é…ç½®
    base_config = CrawlerRunConfig(
        deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1, include_external=False),
        scraping_strategy=LXMLWebScrapingStrategy(),
        verbose=False,
    )

    async with AsyncWebCrawler() as crawler:
        # éæµå¼æ¨¡å¼
        print("\nğŸ“Š éæµå¼æ¨¡å¼:")
        print("  åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰ç»“æœè¢«æ”¶é›†å®Œæˆåæ‰è¿”å›ã€‚")

        non_stream_config = base_config.clone()
        non_stream_config.stream = False

        start_time = time.perf_counter()
        results = await crawler.arun(
            url="https://docs.crawl4ai.com", config=non_stream_config
        )

        print(f"  âœ… ä¸€æ¬¡æ€§æ¥æ”¶æ‰€æœ‰ {len(results)} ä¸ªç»“æœ")
        print(f"  âœ… æ€»è€—æ—¶: {time.perf_counter() - start_time:.2f} ç§’")

        # æµå¼æ¨¡å¼
        print("\nğŸ“Š æµå¼æ¨¡å¼:")
        print("  åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œç»“æœå¯ç”¨æ—¶ç«‹å³å¤„ç†ã€‚")

        stream_config = base_config.clone()
        stream_config.stream = True

        start_time = time.perf_counter()
        result_count = 0
        first_result_time = None

        async for result in await crawler.arun(
            url="https://docs.crawl4ai.com", config=stream_config
        ):
            result_count += 1
            if result_count == 1:
                first_result_time = time.perf_counter() - start_time
                print(
                    f"  âœ… ç¬¬ä¸€ä¸ªç»“æœåœ¨ {first_result_time:.2f} ç§’åæ¥æ”¶: {result.url}"
                )
            elif result_count % 5 == 0:  # ä¸ºç®€æ´èµ·è§ï¼Œæ¯5ä¸ªç»“æœæ˜¾ç¤ºä¸€æ¬¡
                print(f"  â†’ ç»“æœ #{result_count}: {result.url}")

        print(f"  âœ… æ€»è®¡: {result_count} ä¸ªç»“æœ")
        print(f"  âœ… ç¬¬ä¸€ä¸ªç»“æœ: {first_result_time:.2f} ç§’")
        print(f"  âœ… æ‰€æœ‰ç»“æœ: {time.perf_counter() - start_time:.2f} ç§’")
        print("\nğŸ” å…³é”®è¦ç‚¹: æµå¼å…è®¸ç«‹å³å¤„ç†ç»“æœ")

# 3ï¸âƒ£ ä»‹ç»è¿‡æ»¤å™¨ä¸è¯„åˆ†å™¨
async def filters_and_scorers():
    """
    ç¬¬ä¸‰éƒ¨åˆ†: æ¼”ç¤ºä½¿ç”¨è¿‡æ»¤å™¨å’Œè¯„åˆ†å™¨è¿›è¡Œæ›´æœ‰é’ˆå¯¹æ€§çš„çˆ¬å–ã€‚

    æœ¬å‡½æ•°é€æ­¥æ·»åŠ ï¼š
    1. å•ä¸ªURLæ¨¡å¼è¿‡æ»¤å™¨
    2. é“¾ä¸­çš„å¤šä¸ªè¿‡æ»¤å™¨
    3. ç”¨äºé¡µé¢ä¼˜å…ˆçº§æ’åºçš„è¯„åˆ†å™¨
    """
    print("\n===== è¿‡æ»¤å™¨ä¸è¯„åˆ†å™¨ =====")

    async with AsyncWebCrawler() as crawler:
        # å•ä¸ªè¿‡æ»¤å™¨ç¤ºä¾‹
        print("\nğŸ“Š ç¤ºä¾‹ 1: å•ä¸ªURLæ¨¡å¼è¿‡æ»¤å™¨")
        print("  åªçˆ¬å–URLä¸­åŒ…å«'core'çš„é¡µé¢")

        # åˆ›å»ºä¸€ä¸ªåªå…è®¸åŒ…å«'guide'çš„URLçš„è¿‡æ»¤å™¨
        url_filter = URLPatternFilter(patterns=["*core*"])

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1,
                include_external=False,
                filter_chain=FilterChain([url_filter]),  # å•ä¸ªè¿‡æ»¤å™¨
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            cache_mode=CacheMode.BYPASS,
            verbose=True,
        )

        results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

        print(f"  âœ… çˆ¬å–äº† {len(results)} ä¸ªåŒ¹é… '*core*' çš„é¡µé¢")
        for result in results[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
            print(f"  â†’ {result.url}")
        if len(results) > 3:
            print(f"  ... è¿˜æœ‰ {len(results) - 3} ä¸ª")

        # å¤šä¸ªè¿‡æ»¤å™¨ç¤ºä¾‹
        print("\nğŸ“Š ç¤ºä¾‹ 2: é“¾ä¸­çš„å¤šä¸ªè¿‡æ»¤å™¨")
        print("  åªçˆ¬å–æ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„é¡µé¢ï¼š")
        print("  1. URLä¸­åŒ…å«'2024'")
        print("  2. æ¥è‡ª'techcrunch.com'")
        print("  3. å†…å®¹ç±»å‹ä¸º text/html æˆ– application/javascript")

        # åˆ›å»ºè¿‡æ»¤å™¨é“¾
        filter_chain = FilterChain(
            [
                URLPatternFilter(patterns=["*2024*"]),
                DomainFilter(
                    allowed_domains=["techcrunch.com"],
                    blocked_domains=["guce.techcrunch.com", "oidc.techcrunch.com"],
                ),
                ContentTypeFilter(
                    allowed_types=["text/html", "application/javascript"]
                ),
            ]
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1, include_external=False, filter_chain=filter_chain
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
        )

        results = await crawler.arun(url="https://techcrunch.com", config=config)

        print(f"  âœ… åº”ç”¨æ‰€æœ‰è¿‡æ»¤å™¨åçˆ¬å–äº† {len(results)} ä¸ªé¡µé¢")
        for result in results[:3]:
            print(f"  â†’ {result.url}")
        if len(results) > 3:
            print(f"  ... è¿˜æœ‰ {len(results) - 3} ä¸ª")

        # è¯„åˆ†å™¨ç¤ºä¾‹
        print("\nğŸ“Š ç¤ºä¾‹ 3: ä½¿ç”¨å…³é”®è¯ç›¸å…³æ€§è¯„åˆ†å™¨")
        print(
            "æ ¹æ®ä¸ä»¥ä¸‹å…³é”®è¯çš„ç›¸å…³æ€§å¯¹é¡µé¢è¯„åˆ†ï¼š'crawl', 'example', 'async', 'configuration','javascript','css'"
        )

        # åˆ›å»ºå…³é”®è¯ç›¸å…³æ€§è¯„åˆ†å™¨
        keyword_scorer = KeywordRelevanceScorer(
            keywords=["crawl", "example", "async", "configuration","javascript","css"], weight=1
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BestFirstCrawlingStrategy(  
                max_depth=1, include_external=False, url_scorer=keyword_scorer
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            cache_mode=CacheMode.BYPASS,
            verbose=True,
            stream=True,
        )

        results = []
        async for result in await crawler.arun(
            url="https://docs.crawl4ai.com", config=config
        ):
            results.append(result)
            score = result.metadata.get("score")
            print(f"  â†’ Score: {score:.2f} | {result.url}")

        print(f"  âœ… çˆ¬è™«æ ¹æ®ç›¸å…³æ€§è¯„åˆ†ä¼˜å…ˆå¤„ç†äº† {len(results)} ä¸ªé¡µé¢")
        print("  ğŸ” æ³¨æ„: BestFirstCrawlingStrategy ä¼˜å…ˆè®¿é—®è¯„åˆ†æœ€é«˜çš„é¡µé¢")

# 4ï¸âƒ£ é«˜çº§è¿‡æ»¤å™¨
async def advanced_filters():
    """
    ç¬¬å››éƒ¨åˆ†: æ¼”ç¤ºä¸“ä¸šçˆ¬å–çš„é«˜çº§è¿‡æ»¤æŠ€æœ¯ã€‚

    æœ¬å‡½æ•°æ¶µç›–ï¼š
    - SEOè¿‡æ»¤å™¨
    - æ–‡æœ¬ç›¸å…³æ€§è¿‡æ»¤
    - ç»„åˆé«˜çº§è¿‡æ»¤å™¨
    """
    print("\n===== ADVANCED FILTERS =====")

    async with AsyncWebCrawler() as crawler:
        # SEO FILTER EXAMPLE
        print("\nğŸ“Š EXAMPLE 1: SEO FILTERS")
        print(
            "Quantitative SEO quality assessment filter based searching keywords in the head section"
        )

        seo_filter = SEOFilter(
            threshold=0.5, keywords=["dynamic", "interaction", "javascript"]
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1, filter_chain=FilterChain([seo_filter])
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
        )

        results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

        print(f"  âœ… Found {len(results)} pages with relevant keywords")
        for result in results:
            print(f"  â†’ {result.url}")

        # ADVANCED TEXT RELEVANCY FILTER
        print("\nğŸ“Š EXAMPLE 2: ADVANCED TEXT RELEVANCY FILTER")

        # More sophisticated content relevance filter
        relevance_filter = ContentRelevanceFilter(
            query="Interact with the web using your authentic digital identity",
            threshold=0.7,
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1, filter_chain=FilterChain([relevance_filter])
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
        )

        results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

        print(f"  âœ… æ‰¾åˆ° {len(results)} ä¸ªé¡µé¢")
        for result in results:
            relevance_score = result.metadata.get("relevance_score", 0)
            print(f"  â†’ è¯„åˆ†: {relevance_score:.2f} | {result.url}")

# 5ï¸âƒ£ æœ€å¤§é¡µé¢æ•°å’Œè¯„åˆ†é˜ˆå€¼
async def max_pages_and_thresholds():
    """
    ç¬¬äº”éƒ¨åˆ†: æ¼”ç¤ºåœ¨ä¸åŒç­–ç•¥ä¸­ä½¿ç”¨ max_pages å’Œ score_threshold å‚æ•°ã€‚
    
    æœ¬å‡½æ•°å±•ç¤ºï¼š
    - å¦‚ä½•é™åˆ¶çˆ¬å–çš„é¡µé¢æ•°é‡
    - å¦‚ä½•è®¾ç½®è¯„åˆ†é˜ˆå€¼ä»¥å®ç°æ›´æœ‰é’ˆå¯¹æ€§çš„çˆ¬å–
    - æ¯”è¾ƒ BFSã€DFS å’Œ Best-First ç­–ç•¥åœ¨è¿™äº›å‚æ•°ä¸‹çš„è¡¨ç°
    """
    print("\n===== æœ€å¤§é¡µé¢æ•°å’Œè¯„åˆ†é˜ˆå€¼ =====")
    
    from crawl4ai.deep_crawling import DFSDeepCrawlStrategy
    
    async with AsyncWebCrawler() as crawler:
        # ä¸ºæ‰€æœ‰ç¤ºä¾‹å®šä¹‰ä¸€ä¸ªé€šç”¨çš„å…³é”®è¯è¯„åˆ†å™¨
        keyword_scorer = KeywordRelevanceScorer(
            keywords=["browser", "crawler", "web", "automation"], 
            weight=1.0
        )
        
        # ç¤ºä¾‹ 1: å¸¦æœ€å¤§é¡µé¢é™åˆ¶çš„BFS
        print("\nğŸ“Š ç¤ºä¾‹ 1: å¸¦æœ€å¤§é¡µé¢é™åˆ¶çš„BFSç­–ç•¥")
        print("  å°†çˆ¬è™«é™åˆ¶ä¸ºæœ€å¤š5ä¸ªé¡µé¢")
        
        bfs_config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=2, 
                include_external=False,
                url_scorer=keyword_scorer,
                max_pages=1  # åªçˆ¬å–5ä¸ªé¡µé¢
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
        )
        
        results = await crawler.arun(url="https://docs.crawl4ai.com", config=bfs_config)
        
        print(f"  âœ… æŒ‰ç…§max_pagesè®¾ç½®ç²¾ç¡®çˆ¬å–äº† {len(results)} ä¸ªé¡µé¢")
        for result in results:
            depth = result.metadata.get("depth", 0)
            print(f"  â†’ æ·±åº¦: {depth} | {result.url}")
            
        # ç¤ºä¾‹ 2: å¸¦è¯„åˆ†é˜ˆå€¼çš„DFS
        print("\nğŸ“Š ç¤ºä¾‹ 2: å¸¦è¯„åˆ†é˜ˆå€¼çš„DFSç­–ç•¥")
        print("  åªçˆ¬å–ç›¸å…³æ€§è¯„åˆ†é«˜äº0.5çš„é¡µé¢")
        
        dfs_config = CrawlerRunConfig(
            deep_crawl_strategy=DFSDeepCrawlStrategy(
                max_depth=2,
                include_external=False, 
                url_scorer=keyword_scorer,
                score_threshold=0.7,  # åªå¤„ç†è¯„åˆ†é«˜äº0.5çš„URL
                max_pages=10
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
        )
        
        results = await crawler.arun(url="https://docs.crawl4ai.com", config=dfs_config)
        
        print(f"  âœ… çˆ¬å–äº† {len(results)} ä¸ªè¯„åˆ†é«˜äºé˜ˆå€¼çš„é¡µé¢")
        for result in results:
            score = result.metadata.get("score", 0)
            depth = result.metadata.get("depth", 0)
            print(f"  â†’ æ·±åº¦: {depth} | è¯„åˆ†: {score:.2f} | {result.url}")
            
        # ç¤ºä¾‹ 3: å¸¦åŒé‡é™åˆ¶çš„æœ€ä½³ä¼˜å…ˆç­–ç•¥
        print("\nğŸ“Š ç¤ºä¾‹ 3: å¸¦åŒé‡é™åˆ¶çš„æœ€ä½³ä¼˜å…ˆç­–ç•¥")
        print("  é™åˆ¶ä¸º7ä¸ªè¯„åˆ†é«˜äº0.3çš„é¡µé¢ï¼Œä¼˜å…ˆå¤„ç†è¯„åˆ†æœ€é«˜çš„é¡µé¢")
        
        bf_config = CrawlerRunConfig(
            deep_crawl_strategy=BestFirstCrawlingStrategy(
                max_depth=2,
                include_external=False,
                url_scorer=keyword_scorer,
                max_pages=7,          # æ€»å…±é™åˆ¶ä¸º7ä¸ªé¡µé¢
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
            stream=True,
        )
        
        results = []
        async for result in await crawler.arun(url="https://docs.crawl4ai.com", config=bf_config):
            results.append(result)
            score = result.metadata.get("score", 0)
            depth = result.metadata.get("depth", 0)
            print(f"  â†’ æ·±åº¦: {depth} | è¯„åˆ†: {score:.2f} | {result.url}")
            
        print(f"  âœ… çˆ¬å–äº† {len(results)} ä¸ªè¯„åˆ†é«˜äº0.3çš„é«˜ä»·å€¼é¡µé¢")
        if results:
            avg_score = sum(r.metadata.get('score', 0) for r in results) / len(results)
            print(f"  âœ… å¹³å‡è¯„åˆ†: {avg_score:.2f}")
            print("  ğŸ” æ³¨æ„: BestFirstCrawlingStrategy ä¼˜å…ˆè®¿é—®è¯„åˆ†æœ€é«˜çš„é¡µé¢")

# 6ï¸âƒ£ æ€»ç»“ä¸å…³é”®è¦ç‚¹
async def wrap_up():
    """
    ç¬¬å…­éƒ¨åˆ†: æ€»ç»“ä¸å…³é”®è¦ç‚¹

    æ€»ç»“æœ¬æ•™ç¨‹ä¸­å­¦åˆ°çš„å…³é”®æ¦‚å¿µã€‚
    """
    print("\n===== å®Œæ•´çˆ¬è™«ç¤ºä¾‹ =====")
    print("ç»„åˆè¿‡æ»¤å™¨ã€è¯„åˆ†å™¨å’Œæµå¼å¤„ç†ä»¥å®ç°ä¼˜åŒ–çš„çˆ¬å–")

    # åˆ›å»ºå¤æ‚çš„è¿‡æ»¤å™¨é“¾
    filter_chain = FilterChain(
        [
            DomainFilter(
                allowed_domains=["docs.crawl4ai.com"],
                blocked_domains=["old.docs.crawl4ai.com"],
            ),
            URLPatternFilter(patterns=["*core*", "*advanced*", "*blog*"]),
            ContentTypeFilter(allowed_types=["text/html"]),
        ]
    )

    # åˆ›å»ºç»„åˆå¤šç§è¯„åˆ†ç­–ç•¥çš„å¤åˆè¯„åˆ†å™¨
    keyword_scorer = KeywordRelevanceScorer(
        keywords=["crawl", "example", "async", "configuration"], weight=0.7
    )
    # è®¾ç½®é…ç½®
    config = CrawlerRunConfig(
        deep_crawl_strategy=BestFirstCrawlingStrategy(
            max_depth=1,
            include_external=False,
            filter_chain=filter_chain,
            url_scorer=keyword_scorer,
        ),
        scraping_strategy=LXMLWebScrapingStrategy(),
        stream=True,
        verbose=True,
    )

    # æ‰§è¡Œçˆ¬å–
    results = []
    start_time = time.perf_counter()

    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun(
            url="https://docs.crawl4ai.com", config=config
        ):
            results.append(result)
            score = result.metadata.get("score", 0)
            depth = result.metadata.get("depth", 0)
            print(f"â†’ æ·±åº¦: {depth} | è¯„åˆ†: {score:.2f} | {result.url}")

    duration = time.perf_counter() - start_time

    # æ€»ç»“ç»“æœ
    print(f"\nâœ… åœ¨ {duration:.2f} ç§’å†…çˆ¬å–äº† {len(results)} ä¸ªé«˜ä»·å€¼é¡µé¢")
    print(
        f"âœ… å¹³å‡è¯„åˆ†: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}"
    )

    # æŒ‰æ·±åº¦åˆ†ç»„
    depth_counts = {}
    for result in results:
        depth = result.metadata.get("depth", 0)
        depth_counts[depth] = depth_counts.get(depth, 0) + 1

    print("\nğŸ“Š æŒ‰æ·±åº¦ç»Ÿè®¡çš„çˆ¬å–é¡µé¢:")
    for depth, count in sorted(depth_counts.items()):
        print(f"  Depth {depth}: {count} pages")


async def run_tutorial():
    """
    æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰æ•™ç¨‹éƒ¨åˆ†ã€‚
    """
    print("\nğŸš€ CRAWL4AI æ·±åº¦çˆ¬å–æ•™ç¨‹ ğŸš€")
    print("======================================")
    print("æœ¬æ•™ç¨‹å°†å¸¦ä½ äº†è§£ä½¿ç”¨Crawl4AIåº“çš„æ·±åº¦çˆ¬å–æŠ€æœ¯ï¼Œ")
    print("ä»åŸºç¡€åˆ°é«˜çº§åº”ç”¨ã€‚")

    # å®šä¹‰æ•™ç¨‹éƒ¨åˆ† - åœ¨å¼€å‘æœŸé—´å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œç‰¹å®šéƒ¨åˆ†
    tutorial_sections = [
        basic_deep_crawl,
        stream_vs_nonstream,
        filters_and_scorers,
        max_pages_and_thresholds, 
        advanced_filters,
        wrap_up,
    ]

    for section in tutorial_sections:
        await section()

    print("\nğŸ‰ æ•™ç¨‹å®Œæˆï¼ğŸ‰")
    print("ä½ ç°åœ¨å¯¹ä½¿ç”¨Crawl4AIè¿›è¡Œæ·±åº¦çˆ¬å–æœ‰äº†å…¨é¢çš„ç†è§£ã€‚")
    print("æ›´å¤šä¿¡æ¯è¯·è®¿é—® https://docs.crawl4ai.com")

# ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œæ•™ç¨‹
if __name__ == "__main__":
    asyncio.run(run_tutorial())