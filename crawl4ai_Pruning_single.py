import asyncio
import re
from crawl4ai import AsyncWebCrawler
from crawl4ai import BrowserConfig, CrawlerRunConfig
from crawl4ai import DefaultMarkdownGenerator
from crawl4ai import PruningContentFilter
from crawl4ai import CacheMode


# å¤„ç†URLä½œä¸ºæ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦å¹¶ç¼©çŸ­é•¿åº¦
def sanitize_url_filename(url):
    if not url:
        return "untitled"
    # 1. ç§»é™¤URLåè®®å¤´ï¼ˆhttp://, https://ï¼‰
    url_without_protocol = re.sub(r'^https?://', '', url)
    # 2. ç§»é™¤ä¸èƒ½ç”¨äºæ–‡ä»¶åçš„ç‰¹æ®Šå­—ç¬¦ï¼Œæ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    sanitized = re.sub(r'[\\/*?:"<>|/:.]', "_", url_without_protocol)
    # 3. é™åˆ¶æœ€å¤§é•¿åº¦ï¼ˆWindowsæ–‡ä»¶åæœ€å¤§255å­—ç¬¦ï¼Œè¿™é‡Œç•™æœ‰ä½™é‡ï¼‰
    max_length = 200
    if len(sanitized) > max_length:
        # ä¿ç•™å¼€å¤´å’Œç»“å°¾ç‰¹å¾ï¼Œä¸­é—´æˆªæ–­
        sanitized = sanitized[:150] + "_" + sanitized[-49:]
    return sanitized


# è¿‡æ»¤æ— å…³å†…å®¹çš„å‡½æ•°
def filter_unwanted_content(content):
    """
    è¿‡æ»¤æ‰ä¸éœ€è¦çš„å†…å®¹ï¼Œä¿ç•™é“¾æ¥åç§°ç­‰æœ‰æ•ˆæ–‡æœ¬
    """
    if not content:
        return ""
    
    # 1. ç§»é™¤æŒ‡å®šçš„ç›®æ ‡æ–‡æœ¬ï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé—®é¢˜æœç´¢
    target_text = "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé—®é¢˜æœç´¢"
    content = content.replace(target_text, "")
    
    # 2. ç§»é™¤JSONæ ¼å¼çš„æ— å…³æ•°æ®ï¼ˆåŒ¹é… {"key": value, ...} æ ¼å¼ï¼‰
    json_pattern = r'\{[^}]*"serverDuration"[^}]*\}'
    content = re.sub(json_pattern, "", content, flags=re.DOTALL)
    
    # 3. ç§»é™¤å…¶ä»–å¯èƒ½çš„JSONæ ¼å¼å…ƒæ•°æ®
    meta_json_pattern = r'\{[^{}]*"requestCorrelationId"[^{}]*\}'
    content = re.sub(meta_json_pattern, "", content, flags=re.DOTALL)
    
    # 4. ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œå’Œç©ºæ ¼
    content = re.sub(r'\n\s*\n', '\n', content)  # ç§»é™¤ç©ºè¡Œ
    content = content.strip()
    
    return content


async def crawl_and_save_single_url(crawler, url):
    """çˆ¬å–å•ä¸ªURLå¹¶ä¿å­˜ä¸ºmarkdownæ–‡ä»¶"""
    try:
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.DISABLED,
            markdown_generator=DefaultMarkdownGenerator(
                content_filter=PruningContentFilter(
                    threshold=0.1,
                    threshold_type="fixed"
                ),
                options={
                    "ignore_links": True,  # æ”¹ä¸ºFalseä»¥ä¿ç•™é“¾æ¥æ–‡æœ¬
                    "ignore_images": False,   # å¿½ç•¥å›¾ç‰‡å‡å°‘å¹²æ‰°
                }
            )
        )

        print(f"\næ­£åœ¨çˆ¬å–: {url}")
        result = await crawler.arun(url=url, config=run_config)

        # è¿‡æ»¤æ— å…³å†…å®¹ï¼Œåªä¿ç•™æœ‰æ•ˆæ–‡æœ¬ï¼ˆé“¾æ¥åç§°ï¼‰
        cleaned_content = filter_unwanted_content(result.markdown.fit_markdown)

        # ä½¿ç”¨URLä½œä¸ºæ–‡ä»¶åï¼ˆå¤„ç†åï¼‰
        filename = f"{sanitize_url_filename(url)}.md"

        # ä¿å­˜è¿‡æ»¤åçš„å†…å®¹åˆ°markdownæ–‡ä»¶
        with open(filename, "w", encoding="utf-8") as f:
            f.write(cleaned_content)

        print(f"âœ… å·²å®Œæˆå¹¶ä¿å­˜: {filename}")
        print(f"   è¿‡æ»¤åå†…å®¹é•¿åº¦: {len(cleaned_content)} å­—ç¬¦")

        return True

    except Exception as e:
        print(f"âŒ çˆ¬å– {url} æ—¶å‡ºé”™: {str(e)}")
        return False


async def main():
    # ====================== é…ç½®åŒº ======================
    # åœ¨è¿™é‡Œä¿®æ”¹ä½ è¦çˆ¬å–çš„å•ä¸ªURL
    TARGET_URL = "https://www.baidu.com/s?ie=UTF-8&wd=linux%E6%9F%A5%E7%9C%8B%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE"  # æ›¿æ¢ä¸ºä½ è¦çˆ¬å–çš„å®é™…URL
    # ====================================================

    # é…ç½®æµè§ˆå™¨
    browser_config = BrowserConfig(
        headless=True,
        viewport_width=1280,
        viewport_height=720,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        text_mode=True,  # åªæå–æ–‡æœ¬æ¨¡å¼
    )

    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶çˆ¬å–
    async with AsyncWebCrawler(config=browser_config) as crawler:
        success = await crawl_and_save_single_url(crawler, TARGET_URL)

    if success:
        print("\nğŸ‰ å•ä¸ªURLçˆ¬å–å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜åˆ°å½“å‰ç›®å½•ï¼")
    else:
        print("\nâŒ çˆ¬å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®æˆ–ç½‘ç»œæ˜¯å¦æ­£å¸¸ï¼")


if __name__ == "__main__":
    asyncio.run(main())